<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Personal Site of Nikolaos Karianakis.
-->
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta name="keywords" content="karianakis, deep learning, UCLA, Microsoft, computer vision, machine learning, convolutional neural networks, re-identification" />
<meta name="description" content="karianakis, deep learning, UCLA, Microsoft, computer vision, machine learning, convolutional neural networks, re-identification" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />

<title>Home - Nikolaos Karianakis</title>
<link href="style.css" rel="stylesheet" type="text/css" media="screen" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-58737375-1', 'auto');
  ga('send', 'pageview');
</script>
</head>

<body>
<div id="wrapper">
	<div id="header">
		<div id="logo">
			<h1><a href="#">nikolaos karianakis</a></h1>
		</div>
	</div>
	<!-- end #header -->

	<div id="menu">
		<ul>
			<li class="active"><a href="#">Home</a></li>
            		<li><a href="https://karianakis.github.io/proj/cnn_nuisances/" title="projects">Projects</a></li>
            		<li><a href="https://karianakis.github.io/contact/" title="contact">Contact</a></li>
		</ul>
	</div>
	<!-- end #menu -->
	<br>

    <div id="page">
	<div id="page-bgtop">
	<div id="page-bgbtm">
		<div id="content">
			<div class="post">
				<img style="float: center;" src="images/jpl_width620px.png">
				<div style="clear: both;">&nbsp;</div>

				<h3 class="title"><a href="#">Welcome </a></h3>
				<hr style="height:2px; visibility:hidden;" />
				<div class="entry">
					I am a senior researcher with the Computer Vision group at <a href = "https://www.microsoft.com/en-us/research/">Microsoft</a>. 
					As part of my current role, I lead high-impact engineering and applied research projects. My research spans the broader areas of Computer Vision and Machine Learning, 
					with an expertise in Deep Learning. Selected problems that I am currently working on include <a href = "https://azure.microsoft.com/en-us/services
					/cognitive-services/custom-vision-service/">Custom Vision</a> (Automated Deep Learning) and Deep Learning customization for Aerial Images.<br><br>

					I received my Ph.D. in Computer Science from <a href = "http://www.cs.ucla.edu/">UCLA</a>, as a member of the <a href = "http://vision.ucla.edu/">
					Vision Lab</a> under the supervision of <a href = "http://www.cs.ucla.edu/~soatto/">Stefano Soatto</a>. My <a href="./theses/phd_dissertation.pdf">
					Ph.D. dissertation</a> introduces sampling algorithms to handle nuisances in large-scale visual recognition. Previously, I received my diploma in 
					<a href="http://www.ece.ntua.gr/index.php?lang=en">Electrical and Computer Engineering</a> from <a href="https://www.ntua.gr/en/">NTUA</a>. 
					My thesis was conducted under the guidance of <a href="http://cvsp.cs.ntua.gr/maragos/index.shtm">Petros Maragos</a> and proposed Computer 
					Vision algorithms for the digital restoration of the prehistoric <a href="http://en.wikipedia.org/wiki/Wall_Paintings_of_Thera">Wall Paintings of Thera</a>.<br><br>

					<a href="./Resume.pdf">CV</a> / 
					<a href="https://www.linkedin.com/in/nikolaos-karianakis-4016242b?trk=nav_responsive_tab_profile">LinkedIn</a> / 
					<a href="https://scholar.google.com/citations?user=gopF7iMAAAAJ&hl=en&oi=ao">Google Scholar</a><br><br>

					<!--
					<script src="//platform.linkedin.com/in.js" type="text/javascript"></script>
					<script type="IN/MemberProfile" data-id="http://www.linkedin.com/in/nikolaos-karianakis-4016242b" data-format="click" data-related="false"></script>
					-->

				<h3 class="title"><a href="#">Internships</a></h3>
 				<hr style="height:2px; visibility:hidden;" />
 				<div class="entry">
					<b>Summer 2016:</b> Research intern in <a href = "https://www.microsoft.com/en-us/research/">Microsoft Research</a> at Redmond.<br style="line-height: 20px" />
					Our research on Person Re-identification from depth introduced a novel temporal-attention principle based on reinforcement learning 
					(<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf">paper</a>). 
					Mentors: <a href = "http://research.microsoft.com/en-us/um/people/zliu/">Zicheng Liu</a> 
					and <a href = "http://dblp.uni-trier.de/pers/hd/c/Chen:Yinpeng">Yinpeng Chen</a>.<br><br>

					<b>Summer 2015:</b> R&D Engineering intern at <a href = "https://www.sony.net">Sony's</a> Intelligent System Technology Dept. in Tokyo.<br style="line-height: 20px" />
					I designed and implemented deep reinforcement learning software for autonomous navigation. 
					My role involved algorithmic design, implementation lead and experimentation on simulated and real data.
					Mentors: Yusuke Watanabe, Akira Nakamura and Kenta Kawamoto.<br><br>

					<b>Summer 2014:</b> Research intern in NASA's <a href = "http://www.jpl.nasa.gov/">Jet Propulsion Laboratory</a>.<br style="line-height: 20px" />
					We invented a novel algorithm for generic object proposals and large-scale detection (<a href="https://arxiv.org/abs/1503.06350">arXiv</a>).
					Mentor: <a href = "https://www-robotics.jpl.nasa.gov/people/Thomas_Fuchs/">Thomas Fuchs</a>.<br><br>

					<b>Summer 2013:</b> Research intern in the Computer Vision lab at <a href = "http://english.pku.edu.cn/">Peking University</a> in Beijing.<br style="line-height: 20px" />
					Research in graphical models with Computer Vision applications. Mentor: <a href = "http://dblp.uni-trier.de/pers/hd/w/Wang:Yizhou">Yizhou Wang</a>.<br><br>

				<h3 class="title"><a href="#">Selected Publications</a></h3>
				<hr style="height:2px; visibility:hidden;" />
				<div class="entry">
                    G. Mittal, C. Liu, N. Karianakis, V. Fragoso, M. Chen and Y. Fu.<span style="float: right"><mark>NEW</mark></span><br>
                    <b>Hyper-STAR: Task-Aware Hyperparameters for Deep Networks.</b><br>
                    Conference on Computer Vision and Pattern Recognition (CVPR), June 2020 [<a href="./papers/hyperstar_CVPR20.pdf">paper</a>]<br><br>

					N. Karianakis, Z. Liu, Y. Chen and S. Soatto.<br>
					<b>Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification.</b><br>
					European Conference on Computer Vision (ECCV), September 2018 [<a href="./papers/karianakis_Reinforced_Temporal_Attention_ECCV
					_2018.pdf">paper</a>, <a href="https://karianakis.github.io/proj/reinforced_ta/code.zip">code</a>]<br><br>

					N. Karianakis, J. Dong and S. Soatto.<br>
					<b>An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability.</b><br>
					Conference on Computer Vision and Pattern Recognition (CVPR), June 2016 [<a href="https://karianakis.github.io/proj/cnn_nuisances/">project</a>,
					<a href="https://karianakis.github.io/proj/cnn_nuisances/code.zip">code</a>]<br><br>

					J. Dong, N. Karianakis, D. Davis, J. Hernandez, J. Balzer and S. Soatto.<br>
					<b>Multiview Feature Engineering and Learning.</b><br>
					Conference on Computer Vision and Pattern Recognition (CVPR), June 2015 [<a href="./papers/multi_view.pdf">paper</a>]<br><br>

					S. Soatto, J. Dong and N. Karianakis.<br>
					<b>Visual Scene Representations: Scaling and Occlusion in Convolutional Architectures.</b><br>
					International Conference on Learning Representations (ICLR), workshop, May 2015 [<a href="./papers/representations.pdf">paper</a>]<br><br>

					N. Karianakis, T. Fuchs and S. Soatto.<br>
					<b>Boosting Convolutional Features for Robust Object Proposals</b><br>
					arXiv preprint (<a href="https://arxiv.org/abs/1503.06350">arXiv:1503.06350</a>), March 2015 [<a href="./papers/boosting.pdf">paper</a>]<br><br>

					<!--
					N. Karianakis, Y. Wang and S. Soatto.<br>
					<b>Learning to Discriminate in the Wild: Representation-Learning Network for Nuisance-Invariant Image Comparison.</b><br>
					UCLA CS Technical Report, December 2013 [<a href="./papers/TR_130023.pdf">paper</a>]<br><br>
					-->

					N. Karianakis and P. Maragos.<br>
					<b>An integrated System for Digital Restoration of Prehistoric Theran Wall Paintings.</b><br>
					Conference on Digital Signal Processing (DSP), July 2013 [<a href="./papers/DSP13_paper.pdf">paper</a> 
					/ <a href="./papers/DSP13_slides.pdf">slides</a>]<br><br>


				<h3 class="title"><a href="#">Theses</a></h3>
				<hr style="height:2px; visibility:hidden;" />
				<div class="entry">
					N. Karianakis.<br>
					<b>Sampling Algorithms to Handle Nuisances in Large-Scale Recognition.</b><br>
					Ph.D. Dissertation, 2017 (University of California, Los Angeles) [<a href="./theses/phd_dissertation.pdf">pdf</a>]<br><br>

					N. Karianakis.<br>
					<b>Digital Restoration of Prehistoric Theran Wall Paintings.</b><br>
					Diploma Thesis, 2011 (National Technical University of Athens) [<a href="./theses/thesis.pdf">pdf (in Greek)</a>]<br><br>


				<h3 class="title"><a href="#">Demos </a></h3>
				<hr style="height:2px; visibility:hidden;" />
				<div class="entry">
					J. Dong, X. Fei, N. Karianakis, K. Tsotsos and S. Soatto.<br>
					<b>Visual-Inertial Scene Representations.</b><br>
					CVPR demo session, June 2016 [<a href="./posters/cvpr16_demoPoster.pdf">poster</a>,
					<a href="https://www.youtube.com/watch?v=Rt2jdurowfE&feature=youtu.be">demo</a>]<br><br>


				<h3 class="title"><a href="#">Posters </a></h3>
				<hr style="height:2px; visibility:hidden;" />
				<div class="entry">
					<b>Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification.</b><br>
					ECCV, September 2018 [<a href="./posters/karianakisLCS_eccv18.pdf">pdf</a>]<br><br>

					<b>An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability.</b><br>
					CVPR, June 2016 [<a href="./posters/cvpr16poster_karianakisDS.pdf">pdf</a>]<br><br>

					<b>VL‐SLAM: Real‐Time Visual‐Inertial Navigation and Semantic Mapping.</b><br>
					CVPR demo, June 2016 [<a href="./posters/cvpr16_demoPoster.pdf">pdf</a>]<br><br>

					<b>Learning to Discriminate in the Wild: Representation-Learning Network for Nuisance-Invariant Image Comparison.</b><br>
					UCLA SEAS Tech Forum, February 2014 [<a href="./posters/Tech_Forum_poster.pdf">pdf</a>]<br><br>


				<h3 class="title"><a href="#">Research Projects</a></h3>
				<hr style="height:2px; visibility:hidden;" />
				<div class="entry">
					<b>Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification.</b> [<a href="./papers/karianakis_Reinforced_Temporal_
					Attention_ECCV_2018.pdf">paper</a>, <a href="https://karianakis.github.io/proj/reinforced_ta/code.zip">code</a>]<br><br>

						<img src="proj/reinforced_ta/reinforced_lr.png" /><br><br>
						We address the problem of person re-identification from commodity
						depth sensors. One challenge for depth-based recognition is data
						scarcity. Our first contribution addresses this problem by introducing
						split-rate RGB-to-Depth transfer, which leverages large RGB datasets
						more effectively than popular fine-tuning approaches. Our transfer scheme
						is based on the observation that the model parameters at the bottom
						layers of a deep convolutional neural network can be directly shared
						between RGB and depth data while the remaining layers need to be
						fine-tuned rapidly. Our second contribution enhances re-identification
						for video by implementing temporal attention as a Bernoulli-Sigmoid
						unit acting upon frame-level features. Since this unit is stochastic, the
						temporal attention parameters are trained using reinforcement learning.
						Extensive experiments validate the accuracy of our method in person
						re-identification from depth sequences. Finally, in a scenario where subjects
						wear unseen clothes, we show large performance gains compared to
						a state-of-the-art model which relies on RGB data.<br><br>
						<hr style="border-top: dotted 1px;" /><br>

					<b>An Empirical Evaluation of Current Convolutional Architectures’ Ability to Manage Nuisance Location 
					and Scale Variability.</b> [<a href="./papers/karianakisDS_CVPR16.pdf">paper</a>, 
					<a href="https://karianakis.github.io/proj/cnn_nuisances/code.zip">code</a>,
					<a href="https://karianakis.github.io/proj/cnn_nuisances/">project</a>]<br><br>

						<img src="proj/cnn_nuisances/alexNet-entropy_width620px.png" /><br><br>
						We conduct an empirical study to test the ability of convolutional neural networks (CNNs)
						to reduce the effects of nuisance transformations of the input data, such as location, scale 
						and aspect ratio. We isolate factors by adopting a common convolutional architecture either 
						deployed globally on the image to compute class posterior distributions, or restricted locally 
						to compute class conditional distributions given location, scale and aspect ratios of bounding 
						boxes determined by proposal heuristics. In theory, averaging the latter should yield inferior 
						performance compared to proper marginalization. Yet empirical evidence suggests the converse, 
						leading us to conclude that – at the current level of complexity of convolutional architectures 
						and scale of the data sets used to train them – CNNs are not very effective at marginalizing 
						nuisance variability. We also quantify the effects of context on the overall classification task 
						and its impact on the performance of CNNs, and propose improved sampling techniques for heuristic 
						proposal schemes that improve end-to-end performance to state-of-the-art levels. We test our 
						hypothesis on a classification task using the ImageNet benchmark and on a wide-baseline 
						matching task using the Oxford and Fischer’s datasets.<br><br>
						<hr style="border-top: dotted 1px;" /><br>

					<b>Boosting Convolutional Features for Robust Object Proposals.</b> [<a href="./papers/boosting.pdf">paper</a>]<br><br>

						<img src="images/boosting_width620px.png" /><br><br>
						We present a method to generate object proposals, in the form of bounding boxes in a test image, to be fed to a 
						classifier such as a convolutional neural network (CNN), in order to reduce test time complexity of object detection 
						and classification. We leverage on filters learned in the lower layers of CNNs to design a binary boosting classifier 
						and a linear regressor to discard as many windows as possible that are unlikely to contain objects of interest. We 
						test our method against competing proposal schemes, and end-to-end on the Imagenet detection challenge. We show 
						state-of-the-art performance when at least 1000 proposals per frame are used, at a manageable computational 
						complexity compared to alternate schemes that make heavier use of low-level image processing.<br><br>
						<hr style="border-top: dotted 1px;" /><br>

					<b>Learning to Discriminate in the Wild: Representation-Learning Network for Nuisance-Invariant Image Comparison.</b>
					[<a href="./papers/TR_130023.pdf">paper</a>]<br><br>

						<img src="images/occlusions_width620px.png" /><br><br>
						We test the hypothesis that a representation-learning architecture can train away the nuisance variability present in images,
						owing to noise and changes of viewpoint and illumination. First, we establish the simplest possible classification task, a
						binary classification with no intrinsic variability, which amounts to the determination of co-visibility from different images
						of the same underlying scene. This is the Occlusion Detection problem and the data are typically two sequential, but not
						necessarily consecutive or in order, video frames. Our network, based on the Gated Restricted Boltzmann machine (Gated
						RBM), learns away the nuisance variability appearing on the background scene and the occluder, which are irrelevant with
						occlusions, and in turn is capable of discriminating between co-visible and occluded areas by thresholding a one-dimensional
						semi-metric. Our method, combined with Superpixels, outperforms algorithms using features specifically engineered for
						occlusion detection, such as optical flow, appearance, texture and boundaries. We further challenge our framework with
						another Computer Vision problem, Image Segmentation from a single frame. We cast it as binary classification too, but
						here we also have to deal with the intrinsic variability of the scene objects. We perform boundary detection according to a
						similarity map for all pairs of patches and finally provide a semantic image segmentation by leveraging Normalized Cuts.<br><br>
						<hr style="border-top: dotted 1px;" /><br>

					<b>An Integrated System for Digital Restoration of Prehistoric Theran Wall Paintings.</b>
					[<a href="./papers/DSP13_paper.pdf">paper</a>]<br><br>

						<img src="images/thera_width620px.png" /><br><br>
						We present a computer vision system for robust
						restoration of prehistoric Theran wall paintings, replacing or
						just supporting the work of a specialist. In the case of significant
						information loss on some areas of murals, the local inpainting
						methods are not sufficient for satisfactory restoration. Our
						strategy is to detect an area of relevant semantics, geometry and
						color in another location of the wall paintings, which in turn
						is stitched into the missing area by applying a seamless image
						stitching algorithm. An important part of our digital restoration
						system is the damaged and missing areas detector. It is used in
						combination with total variation inpainting at first for the missing
						area extraction and repair, and secondly for the elimination of
						minor defects on the retrieved part in the non-local inpainting
						mechanism. We propose a morphological algorithm for rough
						detection and we improve upon this approach by incorporating
						edge information. For missing areas with complicated boundaries
						we enhance the detection by using iterated graph cuts.<br><br>
						<hr style="border-top: dotted 1px;" />
				</div>
			</div>

			<!--
			%<div style="float: right; clear: right;"> <a id="mws4857385" href="http://webstats.motigo.com/">
			<img width="80" height="15" border="0" alt="Free counter and web stats" src="http://m1.webstats.motigo.com/n80x15.gif?id=AEoeKQhX08N9T0jstD2QQrDbsWVQ" /></a>
			<script src="http://m1.webstats.motigo.com/c.js?id=4857385&amp%3Blang=EN&amp%3Bi=3" type="text/javascript"></script> </div>

			<div style="float: right; clear: right;">
			<a id="mws4879052" href="http://webstats.motigo.com/">
			<img width="80" height="15" border="0" alt="Free counter and web stats" src="http://m1.webstats.motigo.com/n80x15.gif?id=AEpyzAtMt1b4VnaJ2_73XiuW62Tg" /></a>
			<script src="http://m1.webstats.motigo.com/c.js?id=4879052&amp%3Blang=EN&amp%3Bi=27" type="text/javascript"></script> <div>
			-->
		</div>
	</div>
	</div>
    </div>
    <!-- end #page -->
</div>
</body>
</html>
